{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, Dropout, Conv2d, MaxPool2d\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import gurobipy as gb\n",
    "from gurobipy import GRB\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set CUDA_VISIBLE_DEVICES=0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_dir = os.path.join(os.getcwd(), \"dataGeneration/preprocessed_data_test_nopadding\")\n",
    "\n",
    "with open(os.path.join(train_test_dir, f\"X_test.pkl\"), 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "with open(os.path.join(train_test_dir, f\"y_test.pkl\"), 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "with open(os.path.join(train_test_dir, f\"indices_test.pkl\"), 'rb') as f:\n",
    "    index_test = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "with open(os.path.join(train_test_dir, f\"schedule_test.pkl\"), 'rb') as f:\n",
    "    schedule_test = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "solTime_test = np.load(os.path.join(train_test_dir, \"solTime_test.npy\"))\n",
    "objVal_test = np.load(os.path.join(train_test_dir, \"objVal_test.npy\"))\n",
    "model_test = np.load(os.path.join(train_test_dir, \"model_test.npy\")).astype(\"int32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((X_test[0].shape))\n",
    "print(len(y_test))\n",
    "print(len(index_test))\n",
    "\n",
    "print(solTime_test.shape)\n",
    "print(objVal_test.shape)\n",
    "print(model_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Transformer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestep = X_test[0].shape[1]\n",
    "dimensions = X_test[0].shape[2] # feature size\n",
    "\n",
    "nbTime, nbBus, nbSolar, nbScen = 48, 33, 3, X_test[0].shape[0]\n",
    "\n",
    "charging_station = np.squeeze(pd.read_csv(os.path.join(os.path.join(os.getcwd(), 'systemData'), 'cs_params_variable.csv')).to_numpy())\n",
    "nbCS = len(charging_station)\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'systemData')\n",
    "EV_routes = pd.read_csv(os.path.join(data_dir, 'EV_routes.csv')).to_numpy()\n",
    "nbRoute = EV_routes.shape[0]\n",
    "\n",
    "nbOut = (nbRoute*(nbTime-1) + nbCS*nbTime*2)\n",
    "\n",
    "print(dimensions, nbOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.transpose(0,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:,:x.size(1),:]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dimensions: int, out_dim: int, d_model=256, nhead=4, num_layers=2, dim_feedforward=512, dropout=0.1, ffn=256):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = 64\n",
    "        self.dim_feedforward = 256\n",
    "        self.nhead = 4\n",
    "        self.dp = 0.3\n",
    "        self.ffn = 256\n",
    "\n",
    "        self.embedding = nn.Linear(dimensions, self.d_model)\n",
    "\n",
    "        # self.pos_encoder = PositionalEncoding(self.d_model)\n",
    "\n",
    "        transformer_layer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=self.nhead, dim_feedforward=self.dim_feedforward, dropout=self.dp, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=num_layers)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            # nn.MaxPool1d(self.d_model),\n",
    "            # nn.Flatten(),\n",
    "            nn.Linear(self.d_model, self.ffn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.ffn, self.ffn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.ffn, out_dim),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        # x = self.pos_encoder(x)\n",
    "        # print(x.shape)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x[:,(nbBus*2+nbSolar):, :]\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'batch_size' : 1, # Num samples to average over for gradient updates\n",
    "        'EPOCHS' : 200, # Num times to iterate over the entire dataset\n",
    "        'LEARNING_RATE' : 5e-4, # Learning rate for the optimizer\n",
    "        'WEIGHT_DECAY' : 1e-3, # Weight decay parameter for the Adam optimizer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class coordinationDataset(TensorDataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(coordinationDataset, self).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "        \n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.round(torch.tensor(y, dtype=torch.float32))\n",
    "\n",
    "        return X_tensor, y_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = coordinationDataset(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Transformer(dimensions=dimensions, out_dim=nbOut)\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=config[\"LEARNING_RATE\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asymmetric_loss(predict, target, gamma_neg=0.3, gamma_pos=0, clip=0.0, eps=1e-8, disable_torch_grad_focal_loss=True):\n",
    "\n",
    "    \"\"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: input logits\n",
    "    y: targets (multi-label binarized vector)\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculating Probabilities\n",
    "    x_sigmoid = predict\n",
    "    xs_pos = x_sigmoid\n",
    "    xs_neg = 1 - x_sigmoid\n",
    "\n",
    "    # Asymmetric Clipping\n",
    "    if clip is not None and clip > 0:\n",
    "        xs_neg = (xs_neg + clip).clamp(max=1)\n",
    "\n",
    "    # Basic CE calculation\n",
    "    los_pos = target * torch.log(xs_pos.clamp(min=eps))\n",
    "    los_neg = (1 - target) * torch.log(xs_neg.clamp(min=eps))\n",
    "    loss = los_pos + los_neg\n",
    "\n",
    "    # Asymmetric Focusing\n",
    "    if gamma_neg > 0 or gamma_pos > 0:\n",
    "        if disable_torch_grad_focal_loss:\n",
    "            torch.set_grad_enabled(False)\n",
    "        pt0 = xs_pos * target\n",
    "        pt1 = xs_neg * (1 - target)  # pt = p if t > 0 else 1-p\n",
    "        pt = pt0 + pt1\n",
    "        one_sided_gamma = gamma_pos * target + gamma_neg * (1 - target)\n",
    "        one_sided_w = torch.pow(1 - pt, one_sided_gamma)\n",
    "        if disable_torch_grad_focal_loss:\n",
    "            torch.set_grad_enabled(True)\n",
    "        loss *= one_sided_w\n",
    "\n",
    "    return -loss.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "interval = 15\n",
    "\n",
    "net = Transformer(dimensions=dimensions, out_dim=nbOut)\n",
    "net.load_state_dict(torch.load(os.path.join(os.getcwd(), f\"ML_Model/transformer_coordination_nopadding_{interval}.pth\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test number of feasible solutions\n",
    "# test the model on the test set\n",
    "net.eval()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing of bit accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = 0.5\n",
    "\n",
    "one_accuracy = []\n",
    "zero_accuracy = []\n",
    "bit_accuracy = []\n",
    "running_loss = 0\n",
    "mean_one = []\n",
    "mean_zero = []\n",
    "\n",
    "for j, data in enumerate(test_loader):\n",
    "    \n",
    "    net.eval()\n",
    "    inputs_all, labels_all = data\n",
    "\n",
    "    # do a for loop to perform perdiction for each scenario\n",
    "    output_append = torch.tensor([]).to(device) \n",
    "    gt_append = torch.tensor([]).to(device) \n",
    "\n",
    "    for x in range(nbScen):\n",
    "        \n",
    "        inputs, labels = inputs_all[:,x,:,:].to(device), labels_all[:,x,:,].to(device)       \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        outputs = outputs.flatten().reshape(labels.shape[0],-1,)\n",
    "\n",
    "        output_append = torch.concat((output_append, outputs),dim=1)\n",
    "        gt_append = torch.concat((gt_append, labels),dim=1)\n",
    "\n",
    "    output_append = output_append.reshape(-1,)\n",
    "    gt_append = gt_append.reshape(-1,)\n",
    "\n",
    "    # loss_fn = nn.BCELoss()\n",
    "    # loss = loss_fn(outputs, labels)\n",
    "    # loss = asymmetric_loss(outputs, labels)\n",
    "    # running_loss += loss.item()\n",
    "\n",
    "    # start testing\n",
    "    outputs = (output_append).reshape(-1,)   \n",
    "    outputs_percent = outputs\n",
    "    outputs = torch.where(outputs >= thres, torch.ceil(outputs), torch.floor(outputs)).reshape(-1,)\n",
    "    # outputs = torch.round(outputs)\n",
    "    labels = gt_append.reshape(-1,)\n",
    "\n",
    "    one_labels = torch.where(labels == 1)\n",
    "    zero_labels = torch.where(labels == 0)\n",
    "    \n",
    "    one_outputs = outputs[one_labels]\n",
    "    zero_outputs = outputs[zero_labels]\n",
    "\n",
    "    one_acc = 1 - torch.sum(torch.abs(1 - one_outputs)) / one_outputs.shape[0] # 1 minus percentage of error\n",
    "    zero_acc = 1 - torch.sum(torch.abs(0 - zero_outputs)) / zero_outputs.shape[0]\n",
    "    bit_acc = 1 - torch.sum(torch.abs(outputs - labels)) / labels.shape[0]\n",
    "\n",
    "    one_accuracy.append(one_acc.cpu().detach().numpy())\n",
    "    zero_accuracy.append(zero_acc.cpu().detach().numpy())\n",
    "    bit_accuracy.append(bit_acc.cpu().detach().numpy())\n",
    "\n",
    "    # mean acc\n",
    "    id_1 = torch.where(outputs == 1)\n",
    "    id_0 = torch.where(outputs == 0)\n",
    "\n",
    "    p_1 = outputs_percent[id_1]\n",
    "    p_0 = outputs_percent[id_0]\n",
    "\n",
    "\n",
    "    y_1 = labels[id_1]\n",
    "    y_0 = labels[id_0]\n",
    "\n",
    "    y_1_1 = torch.where(y_1 == 1)\n",
    "    y_1_0 = torch.where(y_1 == 0)\n",
    "    y_0_1 = torch.where(y_0 == 1)\n",
    "    y_0_0 = torch.where(y_0 == 0)\n",
    "\n",
    "    avg_1 = torch.mean(torch.cat((p_1[y_1_1], torch.ones(y_1_0[0].shape[0]).to(device) - p_1[y_1_0])))\n",
    "    avg_0 = torch.mean(torch.cat((p_0[y_0_1], torch.ones(y_0_0[0].shape[0]).to(device) - p_0[y_0_0])))\n",
    "\n",
    "    # avg_1 = torch.mean(torch.cat((p_1[y_1_1],  p_1[y_1_0])))\n",
    "    # avg_0 = torch.mean(torch.cat((p_0[y_0_1], p_0[y_0_0])))\n",
    "\n",
    "    # avg_1 = torch.mean(p_1[y_1_1])\n",
    "    # avg_0 = torch.mean(p_0[y_0_1])\n",
    "\n",
    "    # avg_1 = torch.mean(p_1[y_1_0])\n",
    "    # avg_0 = torch.mean(p_0[y_0_1])\n",
    "\n",
    "    mean_one.append(avg_1.cpu().detach().numpy())\n",
    "    mean_zero.append(avg_0.cpu().detach().numpy())\n",
    "\n",
    "print(\"Average one bit accuracy\", np.mean(one_accuracy))\n",
    "print(\"Average zero bit accuracy\", np.mean(zero_accuracy))\n",
    "print(\"Average bit accuracy\", np.mean(bit_accuracy))\n",
    "print('Loss:', running_loss / len(test_loader))\n",
    "print(np.mean(mean_one), np.mean(mean_zero))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for baseline solving speed (speed cap at 300 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in paramaeters\n",
    "data_dir = os.path.join(os.getcwd(), 'systemData')\n",
    "EV_routes = pd.read_csv(os.path.join(data_dir, 'EV_routes.csv')).to_numpy()\n",
    "bus_params = pd.read_csv(os.path.join(data_dir, 'bus_params.csv')).to_numpy()\n",
    "# EV_schedules = pd.read_csv(os.path.join(data_dir, 'EV_schedules.csv')).to_numpy().astype(\"int32\")\n",
    "\n",
    "nbScen, nbTime, nbBus, nbRoute = 5, 48, bus_params.shape[0], EV_routes.shape[0]\n",
    "traffic = np.zeros(nbTime-1)\n",
    "traffic[14:20] = 1      # from 7-10am \n",
    "traffic[32:40] = 1      # from 4-8pm\n",
    "\n",
    "charging_station = np.squeeze(pd.read_csv(os.path.join(data_dir, 'cs_params_variable.csv')).to_numpy())\n",
    "non_charging_station = np.array([i for i in range(nbBus) if i not in charging_station])\n",
    "nbCS = len(charging_station)\n",
    "\n",
    "normal_nodes =  list(charging_station) + list(range(101,108))\n",
    "virtual_nodes = list(range(201,205))\n",
    "congest_nodes = list(range(301,324))\n",
    "\n",
    "always_arc, normal_arc, congest_arc = [], [], []\n",
    "\n",
    "for r in range(nbRoute):\n",
    "    if (EV_routes[r,1] in (normal_nodes+virtual_nodes)) and (EV_routes[r,2] in (normal_nodes+virtual_nodes)) and (EV_routes[r,1] != EV_routes[r,2]):\n",
    "        normal_arc.append(r)\n",
    "    elif (EV_routes[r,1] in (normal_nodes+virtual_nodes)) and (EV_routes[r,2] in congest_nodes):\n",
    "        congest_arc.append(r)\n",
    "    else:\n",
    "        always_arc.append(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gurobi_env = gb.Env()\n",
    "gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "    \n",
    "# loop through all test models and calculate average optimization time\n",
    "opt_time = []\n",
    "opt_val = []\n",
    "opt_ones = []\n",
    "for i, _ in enumerate(model_test):\n",
    "    \n",
    "    runtime = solTime_test[i]\n",
    "    obj = objVal_test[i]\n",
    "    \n",
    "    print(\"Optimization time for model \", i, \": \", runtime)\n",
    "    print(\"Optimization Value for model \", i, \": \", obj)\n",
    "    \n",
    "    opt_time.append(runtime)\n",
    "    opt_val.append(obj)\n",
    "\n",
    "    nbEV = np.max(schedule_test[i][:,0]) + 1\n",
    "    binary_vars = y_test[i][0:(nbRoute*(nbTime-1) + nbCS*nbTime*2)*nbEV]\n",
    "\n",
    "    opt_ones.append(np.count_nonzero(np.round(binary_vars)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_dict = {\n",
    "    \"opt_time\": opt_time,\n",
    "    \"opt_val\": opt_val,\n",
    "    \"opt_ones\": opt_ones\n",
    "}\n",
    "\n",
    "result_path = os.path.join(os.getcwd(), f\"Results\")\n",
    "with open(os.path.join(result_path, \"opt_test_nopadding.pkl\"), 'wb') as f:\n",
    "    pickle.dump(opt_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = os.path.join(os.getcwd(), f\"Results\")\n",
    "with open(os.path.join(result_path, \"opt_test_nopadding.pkl\"), 'rb') as f:\n",
    "    opt_dict = pickle.load(f)\n",
    "\n",
    "opt_time = opt_dict[\"opt_time\"]\n",
    "opt_val = opt_dict[\"opt_val\"]\n",
    "opt_ones = opt_dict[\"opt_ones\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten opt_time\n",
    "print(\"Average optimization time: \", np.mean(opt_time))\n",
    "opt_time_baseline = np.mean(opt_time)\n",
    "# flatten opt_time\n",
    "print(\"Average optimization value: \", np.mean(opt_val))\n",
    "opt_val_baseline = np.mean(opt_val)\n",
    "print(\"Number of ones: \", opt_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the optimization time for each instance in the test set if we use $\\color{lightblue}\\text{equality constraint}$ to find optimal solution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start testing equality constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "gurobi_env = gb.Env()\n",
    "gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "\n",
    "# loop through all test models and calculate average optimization time\n",
    "opt_time_thres = []\n",
    "opt_val_thres = []\n",
    "opt_ones_thres = []\n",
    "opt_utilized_thres = []\n",
    "for i, x in enumerate(model_test):\n",
    "    \n",
    "    path = os.path.join(os.getcwd(), \"dataGeneration/model_test\")\n",
    "\n",
    "    model = gb.read(os.path.join(path, f\"coordination_{x}.mps\"), env=gurobi_env)\n",
    "    model.setParam(\"OutputFlag\", 0)\n",
    "    model.setParam(\"TimeLimit\", 5*60)\n",
    "\n",
    "    ################# timing the first code block #####################\n",
    "    start = time.time()\n",
    "\n",
    "    # deep learning prediciton\n",
    "    # do a for loop to perform perdiction for each scenario\n",
    "    output_append = torch.tensor([]).to(device) \n",
    "\n",
    "    for s in range(nbScen):\n",
    "        inputs = torch.tensor(np.expand_dims(test_dataset.X[i][s], axis=0), dtype=torch.float32) \n",
    "        inputs = inputs.to(device)    \n",
    "        outputs = net(inputs) \n",
    "\n",
    "        output_append = torch.concat((output_append, outputs),dim=1)\n",
    "\n",
    "    output_append = output_append.reshape(-1,)\n",
    "\n",
    "    ################## end #########################\n",
    "\n",
    "    y_pred_binary =  (output_append).reshape(-1,).cpu().detach().numpy()\n",
    "    # y_pred_binary = y_test[i].flatten()\n",
    "    nbEV = int(y_test[i].shape[1]/nbOut)\n",
    "    print(nbEV)\n",
    "\n",
    "    modelVars = model.getVars()\n",
    "\n",
    "    one_threshold = (np.mean(mean_one))\n",
    "    zero_threshold = (1-np.mean(mean_zero))\n",
    "\n",
    "    bin_id = []\n",
    "    ###### Build the index for the variables ######\n",
    "    for sc in range(nbScen):\n",
    "        for k in range(nbEV):\n",
    "            # for each EV get each attribute and order it based on EV number\n",
    "            for r in range(nbRoute):\n",
    "                for t in range(nbTime-1):\n",
    "                    var = model.getVarByName(f\"EVArcStatus[{sc},{k},{r},{t}]\")\n",
    "\n",
    "                    bin_id.append(var.index)\n",
    "\n",
    "            for c in range(nbCS):\n",
    "                for t in range(nbTime):\n",
    "                    var = model.getVarByName(f\"EVChargeStatus[{sc},{k},{c},{t}]\")\n",
    "\n",
    "                    bin_id.append(var.index)\n",
    "                    \n",
    "            for c in range(nbCS):\n",
    "                for t in range(nbTime):\n",
    "                    var = model.getVarByName(f\"EVDischargeStatus[{sc},{k},{c},{t}]\")\n",
    "\n",
    "                    bin_id.append(var.index)\n",
    "\n",
    "    # use equality constestt from here on\n",
    "    for j in range(len(y_pred_binary)):\n",
    "        if (y_pred_binary[j] >= one_threshold or y_pred_binary[j] <= zero_threshold):\n",
    "            modelVars[bin_id[j]].setAttr(\"LB\", round(y_pred_binary[j]))\n",
    "            modelVars[bin_id[j]].setAttr(\"UB\", round(y_pred_binary[j]))\n",
    "            \n",
    "    end = time.time()\n",
    "    runtime1 = end - start\n",
    "\n",
    "    opt_ones_thres.append(np.count_nonzero(y_pred_binary >= one_threshold))\n",
    "    opt_utilized_thres.append(np.count_nonzero(y_pred_binary >= one_threshold)+  np.count_nonzero(y_pred_binary <= zero_threshold))\n",
    "\n",
    "    ################# timing the second code block #####################\n",
    "    start = time.time()\n",
    "\n",
    "    model.optimize()\n",
    "\n",
    "    end = time.time()\n",
    "    runtime2 = end - start\n",
    "    runtime = runtime1 + runtime2\n",
    "    ################## end #########################\n",
    "    \n",
    "    try:\n",
    "        print(\"Optimization time for model \", i, \": \", model.ObjVal)\n",
    "        print(\"Optimization value for model \", i, \": \", runtime)\n",
    "        opt_time_thres.append(runtime)\n",
    "        opt_val_thres.append(model.ObjVal)\n",
    "    except:\n",
    "        print(\"infeasible\")\n",
    "\n",
    "        # model.computeIIS()\n",
    "\n",
    "        # for c in model.getConstrs():\n",
    "        #     if c.IISConstr: print(f'\\t{c.constrname}: {model.getRow(c)} {c.Sense} {c.RHS}')\n",
    "\n",
    "        # for v in model.getVars():\n",
    "        #     if v.IISLB: print(f'\\t{v.varname} ≥ {v.LB}')\n",
    "        #     if v.IISUB: print(f'\\t{v.varname} ≤ {v.UB}')\n",
    "    \n",
    "    model.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_dict_thres = {\n",
    "    \"opt_time\": opt_time_thres,\n",
    "    \"opt_val\": opt_val_thres,\n",
    "    \"opt_ones\": opt_ones_thres,\n",
    "    \"opt_utilized\": opt_utilized_thres\n",
    "}\n",
    "\n",
    "result_path = os.path.join(os.getcwd(), f\"Results\")\n",
    "with open(os.path.join(result_path, \"transformer_opt_thres_test_nopadding.pkl\"), 'wb') as f:\n",
    "    pickle.dump(opt_dict_thres, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(result_path, \"transformer_opt_thres_test_nopadding.pkl\"), 'rb') as f:\n",
    "    opt_dict_thres = pickle.load(f)\n",
    "\n",
    "opt_time_thres = opt_dict_thres[\"opt_time\"]\n",
    "opt_val_thres = opt_dict_thres[\"opt_val\"]\n",
    "opt_ones_thres = opt_dict_thres[\"opt_ones\"]\n",
    "opt_utilized_thres = opt_dict_thres[\"opt_utilized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten opt_time\n",
    "opt_time_thres_mean = np.mean(opt_time_thres)\n",
    "print(\"Average optimization time: \", opt_time_thres_mean)\n",
    "# flatten opt_time\n",
    "opt_val_thres_mean = np.mean(opt_val_thres)\n",
    "print(\"Average optimization value: \", np.mean(opt_val_thres))\n",
    "print(\"Number of feasible model\", len(opt_time_thres))\n",
    "print(\"Number of ones: \", opt_ones_thres)\n",
    "print(\"Number of variable utilized \", opt_utilized_thres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing equality constraint with feasibility check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in paramaeters\n",
    "data_dir = os.path.join(os.getcwd(), 'systemData')\n",
    "EV_routes = pd.read_csv(os.path.join(data_dir, 'EV_routes.csv')).to_numpy()\n",
    "bus_params = pd.read_csv(os.path.join(data_dir, 'bus_params.csv')).to_numpy()\n",
    "# EV_schedules = pd.read_csv(os.path.join(data_dir, 'EV_schedules.csv')).to_numpy().astype(\"int32\")\n",
    "\n",
    "nbTime, nbBus, nbRoute = 48, bus_params.shape[0], EV_routes.shape[0]\n",
    "traffic = np.zeros(nbTime-1)\n",
    "traffic[14:20] = 1      # from 7-10am \n",
    "traffic[32:40] = 1      # from 4-8pm\n",
    "\n",
    "charging_station = np.squeeze(pd.read_csv(os.path.join(data_dir, 'cs_params_variable.csv')).to_numpy())\n",
    "non_charging_station = np.array([i for i in range(nbBus) if i not in charging_station])\n",
    "nbCS = len(charging_station)\n",
    "\n",
    "normal_nodes =  list(charging_station) + list(range(101,108))\n",
    "virtual_nodes = list(range(201,205))\n",
    "congest_nodes = list(range(301,324))\n",
    "\n",
    "always_arc, normal_arc, congest_arc = [], [], []\n",
    "\n",
    "for r in range(nbRoute):\n",
    "    if (EV_routes[r,1] in (normal_nodes+virtual_nodes)) and (EV_routes[r,2] in (normal_nodes+virtual_nodes)) and (EV_routes[r,1] != EV_routes[r,2]):\n",
    "        normal_arc.append(r)\n",
    "    elif (EV_routes[r,1] in (normal_nodes+virtual_nodes)) and (EV_routes[r,2] in congest_nodes):\n",
    "        congest_arc.append(r)\n",
    "    else:\n",
    "        always_arc.append(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def solution_check(y_pred_binary, model, index_test, EV_schedules):\n",
    "\n",
    "#     variables = model.getVars()\n",
    "#     nbEV = int(EV_schedules.shape[0]/4)\n",
    "\n",
    "#     # ========================================= initialise parameters ===================================================\n",
    "\n",
    "#     EVArcStatus = np.zeros((nbScen, nbEV, nbRoute, nbTime-1))\n",
    "#     EVArcStatusIndex = np.zeros((nbScen, nbEV, nbRoute, nbTime-1))\n",
    "#     EVChargeStatus = np.zeros((nbScen, nbEV, nbCS, nbTime))\n",
    "#     EVChargeStatusIndex = np.zeros((nbScen, nbEV, nbCS, nbTime))\n",
    "#     EVDischargeStatus = np.zeros((nbScen, nbEV, nbCS, nbTime))\n",
    "#     EVDischargeStatusIndex = np.zeros((nbScen, nbEV, nbCS, nbTime))\n",
    "\n",
    "#     id_count = 0\n",
    "#     for sc in range(nbScen):\n",
    "#         for k in range(nbEV):\n",
    "#             # for each EV get each attribute and order it based on EV number\n",
    "#             for r in range(nbRoute):\n",
    "#                 for t in range(nbTime-1):\n",
    "#                     EVArcStatus[sc,k,r,t] = y_pred_binary[id_count]\n",
    "#                     EVArcStatusIndex[sc,k,r,t] = y_pred_binary[id_count]\n",
    "#                     id_count = id_count + 1\n",
    "\n",
    "#             for c in range(nbCS):\n",
    "#                 for t in range(nbTime):\n",
    "#                     EVChargeStatus[sc,k,c,t] = y_pred_binary[id_count]\n",
    "#                     EVChargeStatusIndex[sc,k,c,t] = y_pred_binary[id_count]\n",
    "#                     id_count = id_count + 1\n",
    "\n",
    "#             for c in range(nbCS):\n",
    "#                 for t in range(nbTime):\n",
    "#                     EVDischargeStatus[sc,k,c,t] = y_pred_binary[id_count]\n",
    "#                     EVDischargeStatusIndex[sc,k,c,t] = y_pred_binary[id_count]\n",
    "#                     id_count = id_count + 1\n",
    "\n",
    "#     # ========================================= checking Arc constestts ===================================================\n",
    "#     # set disable arc to 0 and check activated arc\n",
    "#     for sc in range(nbScen):\n",
    "#         for s in range(nbTime-1):\n",
    "            \n",
    "#             activate_arc = (always_arc + congest_arc) if traffic[s] else (always_arc + normal_arc)\n",
    "#             disable_arc = normal_arc if traffic[s] else congest_arc\n",
    "            \n",
    "#             EVArcStatus[sc,:,np.array(disable_arc),s] = 0\n",
    "\n",
    "#         # check for EVArcStatus sum == 1    \n",
    "#         MoveStatus = np.sum(np.round(np.clip(EVArcStatus[sc], 0, None)), axis=1)\n",
    "#         # print(MoveStatus)\n",
    "#         for i in range(MoveStatus.shape[0]):\n",
    "#             for j in range(MoveStatus.shape[1]):\n",
    "\n",
    "#                 activate_arc = (always_arc + congest_arc) if traffic[j] else (always_arc + normal_arc)\n",
    "\n",
    "#                 if MoveStatus[i,j] == 0:\n",
    "#                     EVArcStatus[sc,i,activate_arc,j] = -1\n",
    "#                 elif MoveStatus[i,j] > 1:\n",
    "#                     EVArcStatus[sc,i,:,j] = -1\n",
    "#                     # max = np.argmax(EVArcStatus[i,:,j])\n",
    "#                     # EVArcStatus[i,:,j] = 0\n",
    "#                         # EVArcStatus[i,max,j] = 1\n",
    "\n",
    "#         MoveStatus = np.sum(np.round(np.clip(EVArcStatus[sc], 0, None)), axis=1)\n",
    "#         # print(MoveStatus)\n",
    "\n",
    "#         # check EVArc Continuity\n",
    "#         for k in range(nbEV):\n",
    "#             for s in range(nbTime-2):\n",
    "#                 from_arc = np.max(np.round(EVArcStatus[sc,k,:,s]))\n",
    "#                 to_arc = np.max(np.round(EVArcStatus[sc,k,:,s+1]))\n",
    "\n",
    "#                 if from_arc == 1 and to_arc == 1:\n",
    "#                     from_node = np.argmax(np.round(EVArcStatus[sc,k,:,s]))\n",
    "#                     to_node = np.argmax(np.round(EVArcStatus[sc,k,:,s+1]))\n",
    "\n",
    "#                     activate_arc = (always_arc + congest_arc) if traffic[s] else (always_arc + normal_arc)\n",
    "                    \n",
    "#                     if EV_routes[from_node, 2] != EV_routes[to_node, 1]:\n",
    "#                         EVArcStatus[sc,k,:,s] = -1\n",
    "#                         EVArcStatus[sc,k,:,s+1] = -1\n",
    "#                 else:\n",
    "#                     EVArcStatus[sc,k,:,s] = -1\n",
    "#                     EVArcStatus[sc,k,:,s+1] = -1\n",
    "\n",
    "#         # check schedule\n",
    "#         nbSchedule = EV_schedules.shape[0]\n",
    "#         for k in range(nbSchedule):\n",
    "#             schedule = EV_schedules[k]  # [EV, destination, time]\n",
    "\n",
    "#             route_index = (EV_routes[:,1]==schedule[1]).nonzero()[0] if schedule[2] == 0 else (EV_routes[:,2]==schedule[1]).nonzero()[0]\n",
    "#             time = 0 if schedule[2] == 0 else schedule[2]-1\n",
    "\n",
    "#             activate_arc = (always_arc + congest_arc) if traffic[time] else (always_arc + normal_arc)\n",
    "#             disable_arc = normal_arc if traffic[time] else congest_arc\n",
    "\n",
    "#             MoveStatus = np.sum(np.round(np.clip(EVArcStatus[sc,schedule[0], route_index, time], 0, None)))\n",
    "#             if MoveStatus == 0:\n",
    "#                 EVArcStatus[sc,schedule[0],activate_arc,time] = 0\n",
    "#                 EVArcStatus[sc,schedule[0],route_index,time] = -1\n",
    "#             elif MoveStatus > 1:\n",
    "#                 EVArcStatus[sc,schedule[0],activate_arc,time] = -1\n",
    "#                 # max = np.argmax(EVArcStatus[schedule[0],route_index,time])\n",
    "#                 # EVArcStatus[schedule[0],:,time] = 0\n",
    "#                 # EVArcStatus[schedule[0],max,time] = 1\n",
    "\n",
    "#     # ========================================= checking charge constestts ===================================================\n",
    "#     stationary_index = np.array((EV_routes[:,1] == EV_routes[:,2]).nonzero()[0])\n",
    "#     charging_index = np.array([i for i, e in enumerate(EV_routes[:,1]) if e in charging_station])\n",
    "#     charge_index = [i for i, e in enumerate(stationary_index) if e in charging_index]\n",
    "#     for k in range(nbEV):\n",
    "#         for i, ii in enumerate(stationary_index[charge_index]):\n",
    "#             for s in range(nbTime-1):\n",
    "#                 if not ((np.round(np.clip(EVChargeStatus[sc,k,i,s+1], 0, None)) + np.round(np.clip(EVDischargeStatus[sc,k,i,s+1], 0, None))) <= np.round(np.clip(EVArcStatus[sc,k,ii,s], 0, None))):\n",
    "#                     EVChargeStatus[sc,k,i,s+1] = -1\n",
    "#                     EVDischargeStatus[sc,k,i,s+1] = -1\n",
    "\n",
    "#                     activate_arc = (always_arc + congest_arc) if traffic[s] else (always_arc + normal_arc)\n",
    "#                     EVArcStatus[sc,k,:,s] = -1\n",
    "\n",
    "#     ##### replacing y_predict ######\n",
    "#     corrected_y = []\n",
    "\n",
    "#     for sc in range(nbScen):\n",
    "#         for k in range(nbEV):\n",
    "#             # for each EV get each attribute and order it based on EV number\n",
    "#             for r in range(nbRoute):\n",
    "#                 for t in range(nbTime-1):\n",
    "#                     corrected_y.append(EVArcStatus[sc,k,r,t])\n",
    "\n",
    "#             for c in range(nbCS):\n",
    "#                 for t in range(nbTime):\n",
    "#                     corrected_y.append(EVChargeStatus[sc,k,c,t])\n",
    "\n",
    "#             for c in range(nbCS):\n",
    "#                 for t in range(nbTime):\n",
    "#                     corrected_y.append(EVDischargeStatus[sc,k,c,t])\n",
    "\n",
    "#     return corrected_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "gurobi_env = gb.Env()\n",
    "gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "\n",
    "# loop through all test models and calculate average optimization time\n",
    "opt_time_feas = []\n",
    "opt_val_feas = []\n",
    "opt_ones_feas = []\n",
    "opt_utilized_feas = []\n",
    "for i, x in enumerate(model_test):\n",
    "    path = os.path.join(os.getcwd(), \"dataGeneration/model_test\")\n",
    "\n",
    "    model = gb.read(os.path.join(path, f\"coordination_{x}.mps\"), env=gurobi_env)\n",
    "    model.setParam(\"OutputFlag\", 0)\n",
    "    model.setParam(\"TimeLimit\", 120*60)\n",
    "\n",
    "    ################# timing the first code block #####################\n",
    "    start = time.time()\n",
    "\n",
    "    # deep learning prediciton\n",
    "    # do a for loop to perform perdiction for each scenario\n",
    "    output_append = torch.tensor([]).to(device)\n",
    "\n",
    "    for s in range(nbScen):\n",
    "        inputs = torch.tensor(np.expand_dims(test_dataset.X[i][s], axis=0), dtype=torch.float32) \n",
    "        inputs = inputs.to(device)    \n",
    "        outputs = net(inputs) \n",
    "\n",
    "        output_append = torch.concat((output_append, outputs),dim=1)\n",
    "\n",
    "    output_append = output_append.reshape(-1,)\n",
    "\n",
    "    ################## end #########################\n",
    "\n",
    "    y_pred_binary =  (output_append).reshape(-1,).cpu().detach().numpy()\n",
    "    # y_pred_binary = y_test[i].flatten()\n",
    "    nbEV = int(y_test[i].shape[1]/nbOut)\n",
    "    print(nbEV)\n",
    "\n",
    "    modelVars = model.getVars()\n",
    "\n",
    "    one_threshold = (np.mean(mean_one))\n",
    "    zero_threshold = (1-np.mean(mean_zero))\n",
    "    # print(one_threshold, zero_threshold)\n",
    "\n",
    "    bin_id = []\n",
    "    ##### Build the index for the variables ######\n",
    "    for sc in range(nbScen):\n",
    "        for k in range(nbEV):\n",
    "            # for each EV get each attribute and order it based on EV number\n",
    "            for r in range(nbRoute):\n",
    "                for t in range(nbTime-1):\n",
    "                    var = model.getVarByName(f\"EVArcStatus[{sc},{k},{r},{t}]\")\n",
    "\n",
    "                    bin_id.append(var.index)\n",
    "\n",
    "            for c in range(nbCS):\n",
    "                for t in range(nbTime):\n",
    "                    var = model.getVarByName(f\"EVChargeStatus[{sc},{k},{c},{t}]\")\n",
    "\n",
    "                    bin_id.append(var.index)\n",
    "                    \n",
    "            for c in range(nbCS):\n",
    "                for t in range(nbTime):\n",
    "                    var = model.getVarByName(f\"EVDischargeStatus[{sc},{k},{c},{t}]\")\n",
    "\n",
    "                    bin_id.append(var.index)\n",
    "\n",
    "    # use equality constestt from here on\n",
    "    for j in range(len(y_pred_binary)):\n",
    "        if (y_pred_binary[j] >= one_threshold or y_pred_binary[j] <= zero_threshold):\n",
    "        # if (y_pred_binary[j] >= 0):\n",
    "\n",
    "            modelVars[bin_id[j]].setAttr(\"LB\", round(y_pred_binary[j]))\n",
    "            modelVars[bin_id[j]].setAttr(\"UB\", round(y_pred_binary[j]))\n",
    "            \n",
    "    end = time.time()\n",
    "    runtime1 = end - start\n",
    "\n",
    "    opt_ones_feas.append(np.count_nonzero(y_pred_binary >= one_threshold))\n",
    "    opt_utilized_feas.append(np.count_nonzero(y_pred_binary >= one_threshold)+  np.count_nonzero(y_pred_binary <= zero_threshold))\n",
    "\n",
    "    ################# timing the second code block #####################\n",
    "    start = time.time()\n",
    "\n",
    "    print(\"solving...\")\n",
    "    model.optimize()\n",
    "\n",
    "    end = time.time()\n",
    "    runtime2 = end - start\n",
    "    runtime = runtime1 + runtime2\n",
    "    ################## end #########################\n",
    "    \n",
    "    try:\n",
    "        print(\"Optimization time for model \", i, \": \", model.ObjVal)\n",
    "        print(\"Optimization value for model \", i, \": \", runtime)\n",
    "        opt_time_feas.append(runtime)\n",
    "        opt_val_feas.append(model.ObjVal) \n",
    "\n",
    "    except:\n",
    "        print(\"infeasible, reoptimising\")\n",
    "\n",
    "        model.dispose()\n",
    "\n",
    "        ###### reset model and re-fix the values\n",
    "        model = gb.read(os.path.join(path, f\"coordination_{x}.mps\"), env=gurobi_env)\n",
    "        model.setParam(\"OutputFlag\", 0)\n",
    "        model.setParam(\"TimeLimit\", 120*60)\n",
    "\n",
    "        ################# timing the code block #####################\n",
    "        start = time.time()\n",
    "\n",
    "        one_threshold = (np.mean(mean_one)) + 0.2\n",
    "        zero_threshold = (1 - np.mean(mean_zero))\n",
    "\n",
    "        modelVars = model.getVars()\n",
    "\n",
    "        # use equality constestt from here on\n",
    "        for j in range(len(y_pred_binary)):\n",
    "            if (y_pred_binary[j] >= one_threshold or y_pred_binary[j] <= zero_threshold):\n",
    "            # if (y_pred_binary[j] >= 0):\n",
    "\n",
    "                modelVars[bin_id[j]].setAttr(\"LB\", round(y_pred_binary[j]))\n",
    "                modelVars[bin_id[j]].setAttr(\"UB\", round(y_pred_binary[j]))\n",
    "  \n",
    "        end = time.time()\n",
    "        runtime3 = end - start\n",
    "        \n",
    "        opt_ones_feas.append(np.count_nonzero(y_pred_binary >= one_threshold))\n",
    "        opt_utilized_feas.append(np.count_nonzero(y_pred_binary >= one_threshold)+  np.count_nonzero(y_pred_binary <= zero_threshold))\n",
    "\n",
    "        ################# timing the second code block #####################\n",
    "        start = time.time()\n",
    "\n",
    "        model.optimize()\n",
    "\n",
    "        end = time.time()\n",
    "        runtime4 = end - start\n",
    "        runtime = runtime + runtime3 + runtime4 \n",
    "\n",
    "        try:\n",
    "            print(\"Optimization time for model \", i, \": \", model.ObjVal)\n",
    "            print(\"Optimization value for model \", i, \": \", runtime)\n",
    "            opt_time_feas.append(runtime)\n",
    "            opt_val_feas.append(model.ObjVal) \n",
    "        except:\n",
    "            print('infeasible')\n",
    "            opt_time_feas.append(0)\n",
    "            opt_val_feas.append(0)\n",
    "\n",
    "            # model.computeIIS()\n",
    "\n",
    "            # for c in model.getConstrs():\n",
    "            #     if c.IISConstr: print(f'\\t{c.constrname}: {model.getRow(c)} {c.Sense} {c.RHS}')\n",
    "\n",
    "            # for v in model.getVars():\n",
    "            #     if v.IISLB: print(f'\\t{v.varname} ≥ {v.LB}')\n",
    "            #     if v.IISUB: print(f'\\t{v.varname} ≤ {v.UB}')\n",
    "\n",
    "    \n",
    "    model.dispose()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_dict_feas = {\n",
    "    \"opt_time\": opt_time_feas,\n",
    "    \"opt_val\": opt_val_feas,\n",
    "    \"opt_ones\": opt_ones_feas,\n",
    "    \"opt_utilized\": opt_utilized_feas\n",
    "}\n",
    "\n",
    "result_path = os.path.join(os.getcwd(), f\"Results\")\n",
    "with open(os.path.join(result_path, f\"transformer_opt_feas_test_nopadding_{interval}.pkl\"), 'wb') as f:\n",
    "    pickle.dump(opt_dict_feas, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(result_path, f\"transformer_opt_feas_test_nopadding_{interval}.pkl\"), 'rb') as f:\n",
    "    opt_dict_feas = pickle.load(f)\n",
    "\n",
    "opt_time_feas = opt_dict_feas[\"opt_time\"]\n",
    "opt_val_feas = opt_dict_feas[\"opt_val\"]\n",
    "opt_ones_feas = opt_dict_feas[\"opt_ones\"]\n",
    "opt_utilized_feas = opt_dict_feas[\"opt_utilized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten opt_time\n",
    "opt_time_feas = np.array(opt_time_feas)\n",
    "opt_val_feas = np.array(opt_val_feas)\n",
    "opt_val = np.array(opt_val)\n",
    "\n",
    "infeasible_samples = np.where(opt_time_feas == 0)[0]\n",
    "feasible_sample = np.where(opt_time_feas != 0)[0]\n",
    "print(infeasible_samples)\n",
    "# print(feasible_sample)\n",
    "\n",
    "opt_time_feas[infeasible_samples] = np.array(opt_time)[infeasible_samples]\n",
    "opt_val_feas = opt_val_feas[feasible_sample]\n",
    "opt_val = opt_val[feasible_sample]\n",
    "\n",
    "opt_time_feas_mean = np.mean(opt_time_feas)\n",
    "opt_val_feas_mean = np.mean(opt_val_feas)\n",
    "\n",
    "print(\"Average optimization time: \", np.mean(opt_time_feas))\n",
    "print(\"Average optimization value: \", np.mean(opt_val_feas))\n",
    "print(\"Number of feasible model: \", len(opt_val_feas))\n",
    "# print(\"Number of ones: \", opt_ones_feas)\n",
    "# print(\"Number of variable utilized \", opt_utilized_feas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate time sped up by and optimality difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup_time = 1 - (np.array(opt_time_feas)/np.array(opt_time))\n",
    "obj_loss = ((np.array(opt_val_feas)-np.array(opt_val))/np.array(opt_val))\n",
    "feasible_model_feas = (len(opt_time_feas) - len(infeasible_samples))/ len(model_test) * 100\n",
    "\n",
    "\n",
    "print(f\"Time sped up: {speedup_time*100} %\")\n",
    "print(f\"Optimality Loss: {obj_loss*100} %\")\n",
    "print(f\"Feasible model: {feasible_model_feas} %\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
